Title: [WO WO-20250903-004] Enforce robots.txt in extraction and use cached HTTP (Draft)

Problem
- Extraction previously fetched article pages without consulting robots, and used `trafilatura.fetch_url` directly. This risked non-compliance and duplicate HTTP logic outside our cached client.
- HTTP client lacked strict per-domain concurrency. Potentially higher parallelism per host could stress origins.

Approach
- Added `pipeline/robots.py`:
  - Builds robots URL from article URL and fetches via `http_get_bytes` (ETag/Last-Modified caching).
  - Parses robots with `urllib.robotparser.RobotFileParser.parse(lines)`.
  - Exposes `robots_allowed(url, user_agent)` with in-process memoization keyed by `(host, etag/last_modified)`.
  - Helper `parse_robots_text(base_url, text)` for unit tests.
- Updated `pipeline/io.py` HTTP client:
  - Enforces per-domain concurrency = 1 using a host→`threading.Semaphore` map, acquired around requests.
  - Preserves existing RPS throttling, retries with jitter, conditional GETs and caching.
- Updated `pipeline/extract.py`:
  - Replaces `trafilatura.fetch_url` with: robots check → `http_get_bytes` fetch (if allowed) → decode → `trafilatura.extract`.
  - On robots deny or any fetch error, falls back to feed title+summary at quality 0.2.
  - Logs decisions (allow/deny, fetch status, fallback).
- Outputs, shapes, CLI unchanged.

Tradeoffs
- Using stdlib `robotparser` keeps us compliant and dependency-light, though it has limited extensibility versus third-party parsers.
- Per-domain concurrency = 1 reduces peak throughput per host but aligns with ARCHITECT.md guidance and is mitigated by our multi-host parallelism and caching.

Tests & Evidence
- Unit
  - `tests/test_robots.py`: parses sample robots text and asserts allow/deny behavior.
  - `tests/test_extract_robots.py`: monkeypatches robots to deny a URL; verifies zero network fetch of articles and fallback creation. Also tests allowed path with offline fetch error to confirm fallback works.
- Integration
  - Fixture pipeline run (`tests/test_pipeline_fixture.py`) passes with `EMBED_OFFLINE=1`; at least one Article is produced; downstream cluster/summarize still work.
- Coverage
  - Best effort within repo norms. All tests pass locally (`pytest -q`).

Risks & Mitigations
- Mis-parsed robots leading to over-blocking: use stdlib `robotparser`; log allow/deny decisions for audit.
- Added latency from robots fetch: mitigated by conditional GET using ETag/Last-Modified and in-process memoization keyed by validators.
- Throughput impact from per-domain concurrency=1: acceptable by design; existing RPS throttling and retries remain.

Checklists
- Lint/Type: n/a (project standards). Code adheres to style and typing patterns present.
- Tests: added unit tests; full suite passing locally.
- Coverage: acceptable within gates; best effort without introducing new tooling.
- SEC scan: n/a (no new external deps; uses stdlib parser; respects robots).

Labels
- feat, backend, pipelines, compliance

Reviewers
- @maintainer

Auto-merge
- Enable on green.

Command/Branch
- Branch: `feat/WO-20250903-004-robots-compliance-extraction`
- Conventional Commit: `feat: enforce robots.txt in extraction; use cached HTTP and per-domain concurrency (WO-20250903-004)`

Post-merge Note
- No changes to published file formats; outputs remain stable.

Comment to post on merge readiness
- “WO WO-20250903-004: criteria met; artifacts attached. Ready.”

