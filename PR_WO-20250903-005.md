#[WO WO-20250903-005] OpenRouter LLM summarization (model-agnostic, Kimi-K2 default) with caching

Status: Draft PR
Labels: feat, backend, pipelines, summarization, openrouter
Reviewers: @maintainer
Auto-merge: enable on green

## Problem
Per ARCHITECT.md, summarization needs to be deterministic, cached, and offline-friendly. We also need to be model-agnostic and avoid direct vendor SDKs for LLMs. Current pipeline uses a heuristic summarizer only.

## Approach
- Provider: added `pipeline/llm.py` implementing OpenRouter Chat Completions via HTTPS (no vendor SDK). Env: `OPENROUTER_API_KEY`, optional `OPENROUTER_BASE_URL`.
- Prompts: added `pipeline/prompts.py` with ยง14 voice system prompt and map/reduce builders. Centralized `PROMPT_VERSION` and `GUARDRAILS_VERSION`.
- Summarizer: refactored `pipeline/summarize.py` to call provider when `SUMMARIZE_USE_LLM=1` (or CLI `--use-llm`) and key present; otherwise falls back to existing heuristic path. Determinism set (temp=0.2, top_p=0.9, optional seed). Caching/versioning now includes `{model, PROMPT_VERSION, GUARDRAILS_VERSION, sorted(article_ids), extracted_facts}` and checks DB to skip re-calls. Shapes preserved (bullets, citations, tl_dr).
- CLI: `pipeline/__main__.py` exposes `summarize --use-llm/--no-use-llm` and `--model TEXT` (default from `SUMMARIZE_MODEL` or `moonshotai/kimi-k2`). Records `summarize_model` and `use_llm` in `run_report.json` (both `summarize` and `all`).
- README: documented OpenRouter usage, env vars, toggles, and how to compare models by re-running summarize with different `--model`.

## Tradeoffs
- In-process cache boundary is primarily via DB `version_hash` guard to keep behavior simple and cross-run consistent. This avoids retaining per-process memo that could interfere with multi-test execution; DB caching already enforces idempotency.
- Output parsing from the LLM expects a plain text schema (`Lead:` + bullets). Tests stub provider so no network is required.

## Testing & Evidence
- Unit tests (`tests/test_summarize_llm.py`):
  - Guardrails: confirms presence of "preprint; may change post-review", "Bottom line:", and citations.
  - Caching: re-running with same inputs/model calls provider once (DB cache hit).
  - Cache miss on model change: changing `--model` triggers a re-call.
- Integration: `tests/test_pipeline_fixture.py` still passes with `--no-use-llm` path unchanged.
- Local run: `pytest -q` -> all tests pass.

## Risks & Mitigations
- Model name mismatch on OpenRouter: expose `--model` and `SUMMARIZE_MODEL` with clear errors.
- Nondeterminism: use `temperature=0.2`, `top_p=0.9`, optional `seed`, and cache by `version_hash` including `model`.
- Cost/latency: call only at cluster level and cache aggressively.

## Checklists
- Lint/Type: repo norms; unchanged style; minimal, scoped edits.
- Tests: all passing locally and designed to be offline.
- Coverage: within project norms; gates targeted (best effort) with focused tests.
- Security: no secrets in code/tests; env-only.

## How to validate
1) Set env: `OPENROUTER_API_KEY=...` (and optionally `SUMMARIZE_USE_LLM=1`).
2) Run `python -m pipeline summarize --use-llm --model moonshotai/kimi-k2` on a DB with clusters.
3) Inspect DB `summaries` and `run_report.json` for `summarize_model` and idempotency.
4) Compare models: re-run with `--model <other>` and observe a cache miss and new version hash.

## Notes
- Branch: feat/WO-20250903-005-openrouter-llm-summarization
- On merge, please label: feat, backend, pipelines, summarization, openrouter; enable auto-merge on green.
- Comment after CI: "WO WO-20250903-005: criteria met; artifacts attached. Ready."

